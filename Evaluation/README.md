Welcome! This guide walks you through using the `SimuEval` toolkit to evaluate your simultaneous machine translation system, focusing on latency (Average Lagging) and quality (WER, BLEU) metrics.

---

## Step-by-Step Instructions

### 1. Install Python Dependencies

With your virtual environment activated, install the required libraries:
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END
pip install -r requirements.txt


### 2. Using the SimuEval Class for Evaluation

The core of the toolkit is the SimuEval class. Here's how to use it:

a. Import and Initialize

First, import the class and create an instance:

from simueval import SimuEval

evaluator = SimuEval()

b. Process Sentences with the "update" Method

For each sentence pair (source and its translation process), you need to call the evaluator.update() method. This method records the translation process and calculates per-word delays and Average Lagging (AL) for the current instance.

Parameters for update():

inputs (List[str]): A list of source text segments available at each step of generation. inputs[i] is the source seen when pred_outputs[i] was produced. inputs[-1] should be the full source sentence.

pred_outputs (List[str]): A list of partial translations generated by your model. pred_outputs[i] is the hypothesis at step i. pred_outputs[-1] is the final hypothesis.

gold_text (str): The ground truth (reference) translation.

Example for one sentence:

# Example data for one sentence
source_segments = [
    "This is",
    "This is a source",
    "This is a source sentence"
]
predicted_partials = [
    "Ceci",
    "Ceci est une",
    "Ceci est une phrase source"  # Final prediction
]

reference_translation = "Ceci est une phrase source"

evaluator.update(
    inputs=source_segments,
    pred_outputs=predicted_partials,
    gold_text=reference_translation,
)


Repeat this step for every sentence in your evaluation set.

c. Calculate Overall Quality Metrics

After processing all sentences using update(), you can compute:

Word Error Rate (WER):

# Calculates WER for each sentence and the average
individual_WERs, average_WER = evaluator.calc_WER()

print(f"Individual WER scores for each sentence: {individual_WERs}")
print(f"Average WER across all sentences: {average_WER}")

# These are also stored in:
# evaluator.WERs
# evaluator.avg_WER

BLEU Score (Corpus-level):

# Calculates corpus-level BLEU
corpus_bleu_score = evaluator.calc_sacreBLEU()

print(f"Corpus BLEU score: {corpus_bleu_score}")

# This is also stored in:
# evaluator.bleu

d. Access Latency Metrics (Average Lagging - AL)

Average Lagging (AL) is calculated for each sentence during the update() call and stored in evaluator._AL. This attribute is a list of lists, where each inner list contains the AL score(s) for that sentence (typically one).

To get the average AL across all sentences:

# _AL stores AL scores like: [[al_sent1], [al_sent2_if_valid], []]
all_al_scores = []
for al_list_for_sentence in evaluator._AL:
    if al_list_for_sentence:  # Check if AL was computed (non-empty list)
        all_al_scores.append(al_list_for_sentence[0])

if all_al_scores:
    average_AL = sum(all_al_scores) / len(all_al_scores)
    print(f"Average Lagging (AL) across all sentences: {average_AL}")
    print(f"Individual ALs (as stored): {evaluator._AL}")
else:
    print("No AL scores were calculated (e.g., all inputs had empty delays or zero target length).")

5. Full Usage Example

Here's a complete example demonstrating the workflow:

from simueval import SimuEval

# 1. Initialize evaluator
evaluator = SimuEval()

# --- Sentence 1 Data ---
inputs_s1 = ["Hund", "Hund und", "Hund und Katze"]
preds_s1 = ["Dog", "Dog and", "Dog and cat"]
gold_s1 = "Dog and cat"

# --- Sentence 2 Data ---
inputs_s2 = ["Das ist", "Das ist ein Test", "Das ist ein Test Satz"]
preds_s2 = ["This is", "This is a test", "This is a test sentence indeed"] # Prediction differs
gold_s2 = "This is a test sentence"

# 2. Process sentences
print("Processing Sentence 1...")
evaluator.update(inputs=inputs_s1, pred_outputs=preds_s1, gold_text=gold_s1, tokenizer=None)
print("Processing Sentence 2...")
evaluator.update(inputs=inputs_s2, pred_outputs=preds_s2, gold_text=gold_s2, tokenizer=None)

# 3. Calculate and Print Metrics
print("\n--- Evaluation Results ---")

# WER
individual_WERs, average_WER = evaluator.calc_WER()
print(f"Individual WERs: {individual_WERs}")
print(f"Average WER: {average_WER:.4f}")

# BLEU
corpus_bleu = evaluator.calc_sacreBLEU()
print(f"Corpus BLEU: {corpus_bleu:.2f}")

# Average Lagging (AL)
all_al_scores = []
for al_list in evaluator._AL:
    if al_list:
        all_al_scores.append(al_list[0])

if all_al_scores:
    average_AL = sum(all_al_scores) / len(all_al_scores)
    print(f"Average AL: {average_AL:.4f}")
else:
    print("Average AL: N/A (no valid AL scores)")

print(f"\nDetailed AL scores per sentence (from evaluator._AL): {evaluator._AL}")
print(f"Stored predictions: {evaluator.predictions}")
print(f"Stored golden translations: {evaluator.golden_trans}")