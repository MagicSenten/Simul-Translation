Okay, here's a README.md file for simueval.py in a style similar to the example you provided.

README.md

# SimuEval: Simultaneous Translation Evaluation Toolkit

Welcome! This guide walks you through using the `SimuEval` toolkit to evaluate your simultaneous machine translation system, focusing on latency (Average Lagging) and quality (WER, BLEU) metrics.

---

## Step-by-Step Instructions

### 1. Obtain the Project Files

You'll need two files to get started:
-   `simueval.py`: The main Python script containing the `SimuEval` class and `compute` function.
-   `requirements.txt`: A file listing the necessary Python dependencies.

Ensure these files are in your project directory. If you cloned a repository containing them, navigate to that directory.

```bash
# Example: if files are in a directory named 'simueval_toolkit'
cd path/to/simueval_toolkit

### 2. Create and Activate a Virtual Environment

It's highly recommended to use a virtual environment to manage dependencies:

python -m venv venv
source venv/bin/activate       # macOS/Linux
# venv\Scripts\activate      # Windows PowerShell
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END
3. Install Python Dependencies

With your virtual environment activated, install the required libraries:

pip install -r requirements.txt
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Bash
IGNORE_WHEN_COPYING_END

This will install jiwer (for WER) and sacrebleu (for BLEU).

4. Using the SimuEval Class for Evaluation

The core of the toolkit is the SimuEval class. Here's how to use it:

a. Import and Initialize

First, import the class and create an instance:

from simueval import SimuEval

evaluator = SimuEval()
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END
b. Process Sentences with the update Method

For each sentence pair (source and its translation process), you need to call the evaluator.update() method. This method records the translation process and calculates per-word delays and Average Lagging (AL) for the current instance.

Parameters for update():

inputs (List[str]): A list of source text segments available at each step of generation. inputs[i] is the source seen when pred_outputs[i] was produced. inputs[-1] should be the full source sentence.

pred_outputs (List[str]): A list of partial translations generated by your model. pred_outputs[i] is the hypothesis at step i. pred_outputs[-1] is the final hypothesis.

gold_text (str): The ground truth (reference) translation.

tokenizer: A tokenizer object. Note: The current version of simueval.py primarily uses text.strip().split() for word counting in delay calculation. The tokenizer argument is present but not directly used for this specific part.

Example for one sentence:

# Example data for one sentence
source_segments = [
    "This is",
    "This is a source",
    "This is a source sentence"
]
predicted_partials = [
    "Ceci",
    "Ceci est une",
    "Ceci est une phrase source"  # Final prediction
]
reference_translation = "Ceci est une phrase source"

# The tokenizer is part of the method signature.
# Pass None or your actual tokenizer if used in custom modifications.
evaluator.update(
    inputs=source_segments,
    pred_outputs=predicted_partials,
    gold_text=reference_translation,
    tokenizer=None
)
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

Repeat this step for every sentence in your evaluation set.

c. Calculate Overall Quality Metrics

After processing all sentences using update(), you can compute aggregate quality scores:

Word Error Rate (WER):

# Calculates WER for each sentence and the average
individual_WERs, average_WER = evaluator.calc_WER()

print(f"Individual WER scores for each sentence: {individual_WERs}")
print(f"Average WER across all sentences: {average_WER}")

# These are also stored in:
# evaluator.WERs
# evaluator.avg_WER
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

BLEU Score (Corpus-level):

# Calculates corpus-level BLEU
corpus_bleu_score = evaluator.calc_sacreBLEU()

print(f"Corpus BLEU score: {corpus_bleu_score}")

# This is also stored in:
# evaluator.bleu
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END
d. Access Latency Metrics (Average Lagging - AL)

Average Lagging (AL) is calculated for each sentence during the update() call and stored in evaluator._AL. This attribute is a list of lists, where each inner list contains the AL score(s) for that sentence (typically one).

To get the average AL across all sentences:

# _AL stores AL scores like: [[al_sent1], [al_sent2_if_valid], []]
all_al_scores = []
for al_list_for_sentence in evaluator._AL:
    if al_list_for_sentence:  # Check if AL was computed (non-empty list)
        all_al_scores.append(al_list_for_sentence[0])

if all_al_scores:
    average_AL = sum(all_al_scores) / len(all_al_scores)
    print(f"Average Lagging (AL) across all sentences: {average_AL}")
    print(f"Individual ALs (as stored): {evaluator._AL}")
else:
    print("No AL scores were calculated (e.g., all inputs had empty delays or zero target length).")
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END
5. Full Usage Example

Here's a complete example demonstrating the workflow:

from simueval import SimuEval

# 1. Initialize evaluator
evaluator = SimuEval()

# --- Sentence 1 Data ---
inputs_s1 = ["Hund", "Hund und", "Hund und Katze"]
preds_s1 = ["Dog", "Dog and", "Dog and cat"]
gold_s1 = "Dog and cat"

# --- Sentence 2 Data ---
inputs_s2 = ["Das ist", "Das ist ein Test", "Das ist ein Test Satz"]
preds_s2 = ["This is", "This is a test", "This is a test sentence indeed"] # Prediction differs
gold_s2 = "This is a test sentence"

# 2. Process sentences
print("Processing Sentence 1...")
evaluator.update(inputs=inputs_s1, pred_outputs=preds_s1, gold_text=gold_s1, tokenizer=None)
print("Processing Sentence 2...")
evaluator.update(inputs=inputs_s2, pred_outputs=preds_s2, gold_text=gold_s2, tokenizer=None)

# 3. Calculate and Print Metrics
print("\n--- Evaluation Results ---")

# WER
individual_WERs, average_WER = evaluator.calc_WER()
print(f"Individual WERs: {individual_WERs}")
print(f"Average WER: {average_WER:.4f}")

# BLEU
corpus_bleu = evaluator.calc_sacreBLEU()
print(f"Corpus BLEU: {corpus_bleu:.2f}")

# Average Lagging (AL)
all_al_scores = []
for al_list in evaluator._AL:
    if al_list:
        all_al_scores.append(al_list[0])

if all_al_scores:
    average_AL = sum(all_al_scores) / len(all_al_scores)
    print(f"Average AL: {average_AL:.4f}")
else:
    print("Average AL: N/A (no valid AL scores)")

print(f"\nDetailed AL scores per sentence (from evaluator._AL): {evaluator._AL}")
print(f"Stored predictions: {evaluator.predictions}")
print(f"Stored golden translations: {evaluator.golden_trans}")
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END
6. Using the Standalone compute Function (for AL)

The simueval.py script also provides a standalone compute function if you need to calculate Average Lagging for a single instance and already have the necessary delay information.

from simueval import compute as compute_al_latency

# Example data for one sentence
delays_for_sentence = [1, 1, 2, 2, 3]  # Number of source words read per target word
source_length_words = 3
target_length_words = 5

al_score = compute_al_latency(
    delays=delays_for_sentence,
    source_length=source_length_words,
    target_length=target_length_words
)
print(f"\nStandalone AL calculation for one sentence: {al_score:.4f}")
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END

This concludes the guide on using SimuEval. Adapt the examples to fit your specific data and simultaneous translation system's output format.

IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
IGNORE_WHEN_COPYING_END