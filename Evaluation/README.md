Welcome! This guide walks you through using the `SimuEval` ğŸ” toolkit to evaluate your simultaneous machine translation system, focusing on latency (Average Lagging) and quality (TER, BLEU) metrics.

---

## ğŸ‘£ Step-by-Step Instructions

### 1. Install Python Dependencies

With your virtual environment activated, install the required libraries:

```python
pip install -r requirements.txt
```

---
### 2. Using the SimuEval Class for Evaluation

* Import and Initialize

First, import the class and create an instance:

from simueval import SimuEval

```python
evaluator = SimuEval()
```

* Process Sentences with the "update" Method

For each sentence pair (source and its translation process), you need to call the evaluator.update() method. This method records the translation process and calculates per-word delays and Average Lagging (AL) for the current instance.

Parameters for update():

inputs (List[str]): A list of source text segments available at each step of generation. inputs[i] is the source seen when pred_outputs[i] was produced. inputs[-1] should be the full source sentence.

pred_outputs (List[str]): A list of partial translations generated by your model. pred_outputs[i] is the hypothesis at step i. pred_outputs[-1] is the final hypothesis.

gold_text (str): The ground truth (reference) translation.

> ğŸ’¡ *You can also run `process_data` on pre-generated sentences and their hypotheses (e.g., Simul-Translation/AlignAttOutputs/parsed/baseline_alignatt.json) to generate the results in a single step after the main computation finishes.*

---
## ğŸš€ Usage example for one sentence
```python
source_segments = [
    "This is",
    "This is a source",
    "This is a source sentence"
]
predicted_partials = [
    "Ceci",
    "Ceci est une",
    "Ceci est une phrase source"  # Final prediction
]

reference_translation = "Ceci est une phrase source"

evaluator.update(
    inputs=source_segments,
    pred_outputs=predicted_partials,
    gold_text=reference_translation,
)
```

Repeat this step for every sentence in your evaluation set.



After processing all sentences using update(), you can compute:

* Translation Error Rate (TER) for each sentence and the average TER:

```bash
individual_TERs, average_TER = evaluator.calc_TER()

print(f"Individual TER scores for each sentence: {individual_TERs}")
print(f"Average TER across all sentences: {average_TER}")
```
These results are also stored in `evaluator.TERs` and `evaluator.avg_TER`.

* ScareBLEU Score (Corpus-level):

```bash
corpus_bleu_score = evaluator.calc_sacreBLEU()

print(f"Corpus BLEU score: {corpus_bleu_score}")
```
This result is also stored in `evaluator.bleu`.

* Average Lagging (AL):

To get the average AL across all sentences:

```bash
# _AL stores AL scores like: [[al_sent1], [al_sent2_if_valid], []]
all_al_scores = []
for al_list_for_sentence in evaluator._AL:
    if al_list_for_sentence:  # Check if AL was computed (non-empty list)
        all_al_scores.append(al_list_for_sentence[0])

if all_al_scores:
    average_AL = sum(all_al_scores) / len(all_al_scores)
    print(f"Average Lagging (AL) across all sentences: {average_AL}")
    print(f"Individual ALs (as stored): {evaluator._AL}")
else:
    print("No AL scores were calculated (e.g., all inputs had empty delays or zero target length).")
```
This result is stored in `evaluator._AL.`.

---
## ğŸ“˜ Full Usage Example

Here's a complete example demonstrating the workflow:

from simueval import SimuEval

### 1. Initialize evaluator
```bash
evaluator = SimuEval()
```

### >>> Sentence 1 Data <<<
```bash
inputs_s1 = ["Hund", "Hund und", "Hund und Katze"]
preds_s1 = ["Dog", "Dog and", "Dog and cat"]
gold_s1 = "Dog and cat"
```

### >>> Sentence 2 Data <<<
```python
inputs_s2 = ["Das ist", "Das ist ein Test", "Das ist ein Test Satz"]
preds_s2 = ["This is", "This is a test", "This is a test sentence indeed"] # Prediction differs
gold_s2 = "This is a test sentence"
```

### 2. Process sentences
```python
print("Processing Sentence 1...")
evaluator.update(inputs=inputs_s1, pred_outputs=preds_s1, gold_text=gold_s1, tokenizer=None)
print("Processing Sentence 2...")
evaluator.update(inputs=inputs_s2, pred_outputs=preds_s2, gold_text=gold_s2, tokenizer=None)
```

### 3. Calculate and Print Metrics
```python
print("\n--- Evaluation Results ---")

# TER
individual_TERs, average_TER = evaluator.calc_TER()
print(f"Individual TERs: {individual_TERs}")
print(f"Average TER: {average_TER:.4f}")

# BLEU
corpus_bleu = evaluator.calc_sacreBLEU()
print(f"Corpus BLEU: {corpus_bleu:.2f}")

# Average Lagging (AL)
all_al_scores = []
for al_list in evaluator._AL:
    if al_list:
        all_al_scores.append(al_list[0])

if all_al_scores:
    average_AL = sum(all_al_scores) / len(all_al_scores)
    print(f"Average AL: {average_AL:.4f}")
else:
    print("Average AL: N/A (no valid AL scores)")

print(f"\nDetailed AL scores per sentence (from evaluator._AL): {evaluator._AL}")
print(f"Stored predictions: {evaluator.predictions}")
print(f"Stored golden translations: {evaluator.golden_trans}")
```
